# -*- coding: utf-8 -*-
#
# Copyright (C) 2015 Niklas Rosenstein
#
# Permission is hereby granted, free of charge, to any person obtaining a copy
# of this software and associated documentation files (the "Software"), to deal
# in the Software without restriction, including without limitation the rights
# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
# copies of the Software, and to permit persons to whom the Software is
# furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in
# all copies or substantial portions of the Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
# THE SOFTWARE.
'''
`nr.strex` String Processing Library
====================================

This library provides classes to make the implementation of mini
languages a very easy process. The library was developed with
recursive decent parsing in mind, though it might work well with
other parsing techniques as well.

The `Scanner` is a class to parse text character by character and
keep track of the line and column number and be able to seemlessly
seek in the source text buffer.

The `Lexer` is a class to tokenize text using the `Scanner` class
and a list of `Rule` instances. A `Rule` implementation can use the
`Scanner` to extract a token from the current scanner position.

A `Token` is generated by a `Lexer` and it contains the token type
identifier, its value and the `Cursor` position from which the token
was parsed. An invalid Token is yielded when no `Rule` applied to
match from the current scanners position and the tokens value will
be the single character at that cursor location. The special token
type `'eof'` is used to represent the token at the end of the input
stream.

Some special `Rule` subclasses like `Regex` yield `Token`s for which
the `Token.value` is not a string but a different datatype (eg. the
regex match object).
'''

import collections
import string
import re
import sys

__author__ = 'Niklas Rosenstein <rosensteinniklas(at)gmail.com>'
__version__ = '1.4.0'

eof = 'eof'
string_types = (str,) if sys.version_info[0] == 3 else (str, unicode)

Cursor = collections.namedtuple('Cursor', 'index lineno colno')
Token = collections.namedtuple('Token', 'type cursor value')


class Scanner(object):
  ''' This class is used to step through text character by character
  and keep track of the line and column numbers of each passed
  character. The Scanner will only tread line-feed as a newline.

  @param text
    The text to parse. Must be a `str` in Python 3 and may also be
    a `unicode` object in Python 2.

  @attr text
  @attr index The index in the text.
  @attr lineno The current line number.
  @attr colno The current column number.
  @property cursor The current `Cursor` value.
  @property char The current character, or an empty string/unicode
    if the end of the text was reached. '''

  def __init__(self, text):
    if not isinstance(text, string_types):
      raise TypeError('expected str or unicode', type(text))
    super(Scanner, self).__init__()
    self.text = text
    self.index = 0
    self.lineno = 1
    self.colno = 0

  def __repr__(self):
    return '<Scanner at {}:{}>'.format(self.lineno, self.colno)

  def __bool__(self):
    return self.index < len(self.text)

  __nonzero__ = __bool__  # Python 2

  @property
  def cursor(self):
    return Cursor(self.index, self.lineno, self.colno)

  @property
  def char(self):
    if self.index >= 0 and self.index < len(self.text):
      return self.text[self.index]
    else:
      return type(self.text)()

  def next(self):
    ''' Move on to the next character in the scanned text. '''

    char = self.char
    if char == '\n':
      self.lineno += 1
      self.colno = 0
    else:
      self.colno += 1
    self.index += 1

  def next_get(self):
    ''' Like `next()` but returns the new character. '''

    self.next()
    return self.char

  def restore(self, cursor):
    ''' Moves the scanner back (or forward) to the specified cursor. '''

    if not isinstance(cursor, Cursor):
      raise TypeError('expected Cursor object', type(cursor))
    self.index, self.lineno, self.colno = cursor


def readline(scanner):
  ''' Reads a full line from the *scanner* and returns it. This is
  fast over using `Scanner.next()` to the line-feed. The resulting
  string contains the line-feed character if present. '''

  start = end = scanner.index
  while end < len(scanner.text):
    if scanner.text[end] == '\n':
      end += 1
      break
    end += 1
  result = scanner.text[start:end]
  scanner.index = end
  if result.endswith('\n'):
    scanner.colno = 0
    scanner.lineno += 1
  else:
    scanner.colno += end - start
  return result


def match(scanner, regex, flags=0):
  ''' Matches the specified *regex* from the current character of the
  *scanner* and returns a match object or None if it didn't match. The
  Scanners column and line numbers are updated respectively. '''

  if isinstance(regex, str):
    regex = re.compile(regex, flags)
  match = regex.match(scanner.text, scanner.index)
  if not match:
    return None
  start, end = match.start(), match.end()
  lines = scanner.text.count('\n', start, end)
  scanner.index = end
  if lines:
    scanner.colno = end - scanner.text.rfind('\n', start, end) - 1
    scanner.lineno += lines
  else:
    scanner.colno += end - start
  return match


class Lexer(object):
  ''' This class is used to split text into `Token`s using a
  `Scanner` and a list of `Rule`s. If *raise_invalid* is True, it
  raises an `TokenizationError` instead of yielding an invalid
  `Token` object.

  @param scanner The `Scanner` to use for lexing.
  @param rules A list of `Rule` objects.
  @param raise_invalid True if an exception should be raised when
    the stream can not be tokenized, False if it should just yield
    an invalid token and proceed with the next character.

  @attr scanner
  @attr rules
  @attr raise_invalid
  @attr skip_rules A set of rule type IDs that will automatically
    be skipped by the `next()` method.
  @attr token The current `Token`. After the Lexer is created and
    the `next()` method has not been called, the value of this
    attribute is None. At the end of the input, the token is of
    type `eof`.
  '''

  def __init__(self, scanner, rules=None, raise_invalid=True):
    super(Lexer, self).__init__()
    self.scanner = scanner
    self.rules = list(rules) if rules else []
    self.raise_invalid = raise_invalid
    self.token = None

  def __repr__(self):
    ctok = self.token.type if self.token else None
    return '<Lexer with current token {0!r}>'.format(ctok)

  def __iter__(self):
    if not self.token:
      self.next()
    while not self.token.type == eof:
      yield self.token
      self.next()

  def __bool__(self):
    if self.token and self.token.type == eof:
      return False
    return True

  __nonzero__ = __bool__  # Python 2

  def next(self):
    ''' Parse the next token from the input and return it. If
    `raise_invalid` is True, this method can raise `TokenizationError`.
    The new token can also be accessed from the `token` attribute
    after the method was called. '''

    if self.token and self.token.type == eof:
      return self.token

    token = None
    while token is None:
      cursor = self.scanner.cursor
      if not self.scanner:
        token = Token(eof, cursor, None)
        break

      for rule in self.rules:
        value = rule.tokenize(self.scanner)
        if value:
          if type(value) is not Token:
            token = Token(rule.name, cursor, value)
          break
        self.scanner.restore(cursor)

      if token is None:
        token = Token(None, cursor, self.scanner.char)
        if self.raise_invalid:
          raise TokenizationError(token)
      elif rule.skip:
        token = None

    self.token = token
    return token


class Rule(object):
  ''' Base class for rule objects that are capable of extracting a
  `Token` from the current position of a `Scanner`. '''

  def __init__(self, name, skip=False):
    super(Rule, self).__init__()
    self.name = name
    self.skip = skip

  def tokenize(self, scanner):
    ''' Attempt to extract a token from the position of the *scanner*
    and return it. If a non-`Token` instance is returned, it will be
    used as the tokens value. Any value that evaluates to False will
    make the Lexer assume that the rule couldn't capture a Token.

    The `Token.value` must not necessarily be a string though, it can
    be any data type or even a complex datatype, only the user must
    know about it and handle the tokens special. '''

    raise NotImplementedError


class Regex(Rule):
  ''' A rule to match a regular expression. The `Token` generated by
  this rule contains the match object as its value. '''

  def __init__(self, name, regex, flags=0, skip=False):
    super(Regex, self).__init__(name, skip)
    if isinstance(regex, string_types):
      regex = re.compile(regex, flags)
    self.regex = regex

  def tokenize(self, scanner):
    result = match(scanner, self.regex)
    if result is None or result.start() == result.end():
      return None
    return result


class Keyword(Rule):

  def __init__(self, name, string, case_sensitive=True, skip=False):
    super(Keyword, self).__init__(name, skip)
    self.string = string
    self.case_sensitive = case_sensitive

  def tokenize(self, scanner):
    string = self.string if self.case_sensitive else self.string.lower()
    char = scanner.char
    result = type(char)()
    for other_char in string:
      if not self.case_sensitive:
        char = char.lower()
      if char != other_char:
        return None
      result += char
      char = scanner.next_get()
    return result


class Charset(Rule):

  def __init__(self, name, charset, at_column=-1, skip=False):
    super(Charset, self).__init__(name, skip)
    self.charset = frozenset(charset)
    self.at_column = at_column

  def tokenize(self, scanner):
    if self.at_column >= 0 and self.at_column != scanner.colno:
      return None
    char = scanner.char
    result = type(char)()
    while char and char in self.charset:
      result += char
      char = scanner.next_get()
    return result


class TokenizationError(Exception):

  def __init__(self, token):
    self.token = token

  def __str__(self):
    return str(self.token)
